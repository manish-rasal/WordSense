# -*- coding: utf-8 -*-
"""WordSense.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m4oPskNmvxqIxqa-FlyINz514zNPLhsH

### Task 1: Reading and Extracting Words from a Text File

We'll start by reading a file named "words.txt" and extracting all the words from it. It's crucial to lowercase all the words to maintain consistency and prevent counting the same word multiple times due to case differences. Next, we'll compile a set containing all the unique words found in the file. As a quick check, we'll also display the first ten words from the file. Now, let's begin and explore the content of "words.txt"!
"""

#--- Import path from pathlib,re,counter from collections ---
import re
from pathlib import Path
from collections import Counter

#--- Read in text file (''words.txt'') ----
file_path = Path("../data/words.txt")
with open(file_path, 'r') as file:
    file_content = file.read()

# ---WRITE CODE FOR TASK 1 ---
words = re.findall(r'\w+', file_content.lower())
V = set(words)
ten_words = words[:10]

#--- Inspect data ---
print("Unique vocabulary:", V)
print("First ten words:", ten_words)

"""### Task 2: Counting Word Frequencies

Now, let's tally the frequency of each word in the file. We'll utilize a dictionary to keep track of how often each word occurs. Moreover, we'll identify the ten most common words and present them. Let's proceed with our investigation to uncover which words dominate the text file!
"""

# --- WRITE CODE FOR TASK 2 ---
word_freq = {}
word_freq = Counter(words)
most_common_txt = word_freq.most_common(10)

#--- Inspect data ---
print("Ten most common words and their frequencies:")
for word, freq in most_common_txt:
    print(word, ":", freq)

"""### Task 3: Visualizing Top Word Frequencies

Let's advance our analysis by visualizing the word frequencies in our text data. This visualization will provide insight into the most prevalent words.

Generate a bar plot illustrating the top 20 most frequent words along with their frequencies. This graphical representation will facilitate the comprehension of word distribution within our text.

Let's move forward and produce a bar plot to visualize the frequencies of the top 20 most common words!
"""

#--- Import matplotlib.pyplot as plts ---
import matplotlib.pyplot as plt

# --- WRITE CODE FOR TASK 3 ---
most_common_20 = word_freq.most_common(20)
words, frequencies = zip(*most_common_20)

#--- Inspect data ---
plt.figure(figsize=(10, 6))
plt.bar(words, frequencies)
plt.xlabel('Words')
plt.ylabel('Frequencies')
plt.title('Top 20 Most Common Words and Their Frequencies')
plt.xticks(rotation=45)
plt.show()

"""### Task 4: Calculating Word Probabilities

Now, let's progress to determine the probabilities associated with each word in our text data. This analysis will provide insight into the probability of encountering each word within the text.

We'll compute the probability of each word by dividing its frequency by the total sum of word frequencies. This computation will provide a measure of the relative commonality or rarity of each word across the entire text.

Let's move forward and calculate the probabilities of each word in our text data to gain deeper insights into their occurrences!
"""

# --- WRITE CODE FOR TASK 4 ---
probs = {}

total_sum = sum(word_freq.values())

for word, freq in word_freq.items():
    probs[word] = freq / total_sum

#--- Inspect data ---
print("Probabilities of each word:")
for word, prob in probs.items():
    print(word, ":", prob)

"""### Task 5: Implementing Auto-correction Algorithm

Now, let's extend our analysis by constructing an autocorrect feature. This feature will assist users in rectifying any misspelled words they enter.

We'll examine whether the input word is present in our lexicon. If it is, we'll inform the user that the word appears to be correct. However, if the word is not found in our lexicon, we'll propose some alternative words based on their probability and resemblance to the input word.

Let's continue and develop the autocorrect function to aid users in rectifying their misspelled words!
"""

#--- import textdistance ---
import textdistance
import pandas as pd

# --- WRITE CODE FOR TASK 5 ---
def my_autocorrect(input_word):
    input_word = input_word.lower()

    if input_word in V:
        return "Your word seems to be correct"

    similarity_scores = {}
    for word in word_freq.keys():
        similarity_scores[word] = textdistance.Jaccard(qval=2).normalized_similarity(input_word, word)

    df = pd.DataFrame({'Word': list(probs.keys()), 'Prob': list(probs.values()), 'Similarity': list(similarity_scores.values())})

    df.columns = ['Word', 'Prob', 'Similarity']

    df = df.sort_values(by=['Similarity', 'Prob'], ascending=[False, False])

    return df.head()


suggestion_words = my_autocorrect('neverteless')
#--- Inspect data ---
#suggestion_words
print(suggestion_words)

"""### Task 6: Implementing Enhanced Correction Suggestions

Congratulations on your advancements! Now, let's elevate our word correction functionality to offer more accurate suggestions for misspelled words.

If the input word is already accurate, we'll return it unchanged. However, if the word is not recognized in our vocabulary, we'll propose similar words based on their resemblance to the input word. This time, we'll employ a different technique known as Levenshtein distance to identify similar words.

Let's continue and refine our word correction function to offer improved suggestions for misspelled words!
"""

# --- WRITE YOUR CODE FOR TASK 6 ---

def correction_suggestion(word):
    word = word.lower()

    if word in V:
        return word

    suggestions = [w for w in V if textdistance.levenshtein.normalized_similarity(word, w) > 0.8]

    return suggestions[0] if suggestions else None


result = correction_suggestion('kidness')
#--- Inspect data ---
#result
print(result)